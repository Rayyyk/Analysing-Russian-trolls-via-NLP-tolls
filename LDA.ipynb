{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COMP4560 - Artefact\n",
    "## Implementation of Topic model\n",
    "\n",
    "\n",
    "Bokun Kong, u6342099\n",
    "\n",
    "Supervisor: Dr. Dongwoo Kim\n",
    "\n",
    "This jupyter notebook file is created by Bokun Kong.\n",
    "\n",
    "Dataset of Russian troll: https://github.com/fivethirtyeight/russian-troll-tweets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/Desktop/anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:1003: UserWarning: Duplicate key in file \"/Users/raymond/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/Users/raymond/Desktop/anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:1003: UserWarning: Duplicate key in file \"/Users/raymond/.matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/Desktop/anaconda3/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "/Users/raymond/Desktop/anaconda3/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (0,15,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns of the whole dataset: (2435342, 21)\n"
     ]
    }
   ],
   "source": [
    "# Reading files and extract useful information\n",
    "allfiles = glob.glob('training dataset/IRAhandle_tweets_*.csv')\n",
    "allfiles.sort()\n",
    "\n",
    "data = pd.concat([pd.read_csv(f) for f in allfiles], ignore_index = True)\n",
    "print('Number of rows and columns of the whole dataset: {}'.format(data.shape))\n",
    "\n",
    "\n",
    "df_en = data[(data['language'] == 'English') & (data['account_category'] != 'NonEnglish') & (~data['content'].isnull())]\n",
    "\n",
    "df_lnr = df_en[((df_en['account_category'] == 'LeftTroll') | (df_en['account_category'] == 'RightTroll'))]\n",
    "\n",
    "df_lnr = df_lnr.reset_index(drop=True)\n",
    "\n",
    "# Creat a new dataset containing useful tweets\n",
    "df_lnr.to_csv(\"left_right_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(984045, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the generated dataset\n",
    "df_tweets = pd.read_csv(\"left_right_news.csv\", parse_dates=['publish_date'])\n",
    "df_tweets = df_tweets.drop(['external_author_id', 'author', 'region', 'language', 'harvested_date', 'following', 'followers', 'updates', 'post_type', 'account_type', 'retweet', 'new_june_2018', 'alt_external_id', 'article_url', 'tco1_step1', 'tco2_step1', 'tco3_step1', 'tweet_id'], axis=1)\n",
    "df_tweets['index'] = df_tweets.index\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessor(text):\n",
    "    tokens = WordPunctTokenizer().tokenize(text.lower())\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in gensim.parsing.preprocessing.STOPWORDS and token not in punctuation and len(token) >= 3 and len(token) <= 14:\n",
    "            stems.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "   \n",
    "    return stems\n",
    "\n",
    "docs = df_tweets['content'].map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(40112 unique tokens: ['barely', 'corruption', 'democrat', 'hear', 'mainstream']...)\n"
     ]
    }
   ],
   "source": [
    "# Creating dictionary for corpus\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.filter_n_most_frequent(40)\n",
    "dictionary.filter_extremes(no_below=6, keep_n=100000)\n",
    "\n",
    "print(dictionary)\n",
    "# Generate Document-Term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.36661796692922144),\n",
      " (1, 0.2687982153569831),\n",
      " (2, 0.24648225277166924),\n",
      " (3, 0.22712687135389736),\n",
      " (4, 0.3034109454103004),\n",
      " (5, 0.45942562653400876),\n",
      " (6, 0.37300551049882885),\n",
      " (7, 0.2660573236666947),\n",
      " (8, 0.2698457552997815),\n",
      " (9, 0.30843511314335414)]\n"
     ]
    }
   ],
   "source": [
    "# Implementing TF-IDF\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "# Print\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LDA using Bag of Words\n",
    "# Generate model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "lda_model = gensim.models.LdaMulticore(doc_term_matrix, num_topics=20, id2word=dictionary, iterations=50, decay=0.5, offset=1.0, gamma_threshold=0.001, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.032*\"tcot\" + 0.020*\"pjnet\" + 0.014*\"ccot\" + 0.010*\"wakeupamerica\" + 0.010*\"refugees\" + 0.008*\"islamkills\" + 0.007*\"love\" + 0.007*\"american\" + 0.007*\"come\" + 0.006*\"support\"\n",
      "Topic: 1 \n",
      "Words: 0.016*\"nowplaying\" + 0.014*\"music\" + 0.011*\"play\" + 0.010*\"spend\" + 0.009*\"million\" + 0.009*\"water\" + 0.009*\"best\" + 0.007*\"soundcloud\" + 0.006*\"action\" + 0.006*\"shit\"\n",
      "Topic: 2 \n",
      "Words: 0.022*\"great\" + 0.011*\"anti\" + 0.010*\"gun\" + 0.008*\"demndebate\" + 0.007*\"control\" + 0.007*\"demdebate\" + 0.006*\"attack\" + 0.006*\"blame\" + 0.005*\"fight\" + 0.005*\"muslim\"\n",
      "Topic: 3 \n",
      "Words: 0.018*\"love\" + 0.011*\"live\" + 0.008*\"check\" + 0.007*\"life\" + 0.007*\"sell\" + 0.005*\"god\" + 0.005*\"state\" + 0.005*\"girls\" + 0.005*\"business\" + 0.005*\"food\"\n",
      "Topic: 4 \n",
      "Words: 0.012*\"mind\" + 0.010*\"damn\" + 0.009*\"potus\" + 0.009*\"lose\" + 0.008*\"build\" + 0.007*\"way\" + 0.007*\"day\" + 0.006*\"heart\" + 0.006*\"fear\" + 0.005*\"power\"\n",
      "Topic: 5 \n",
      "Words: 0.011*\"free\" + 0.009*\"racist\" + 0.008*\"speech\" + 0.007*\"school\" + 0.007*\"illegal\" + 0.007*\"money\" + 0.007*\"honor\" + 0.006*\"american\" + 0.005*\"vet\" + 0.005*\"vegas\"\n",
      "Topic: 6 \n",
      "Words: 0.020*\"isis\" + 0.014*\"mueller\" + 0.014*\"target\" + 0.007*\"sheriff\" + 0.007*\"account\" + 0.006*\"report\" + 0.006*\"surprise\" + 0.005*\"ryan\" + 0.005*\"gun\" + 0.005*\"paul\"\n",
      "Topic: 7 \n",
      "Words: 0.012*\"north\" + 0.011*\"deal\" + 0.010*\"korea\" + 0.009*\"fail\" + 0.009*\"iran\" + 0.008*\"mean\" + 0.007*\"hate\" + 0.007*\"take\" + 0.006*\"mccain\" + 0.006*\"ban\"\n",
      "Topic: 8 \n",
      "Words: 0.016*\"watch\" + 0.011*\"thank\" + 0.011*\"good\" + 0.009*\"latest\" + 0.006*\"director\" + 0.006*\"wikileaks\" + 0.006*\"maga\" + 0.006*\"morning\" + 0.006*\"fraud\" + 0.005*\"cia\"\n",
      "Topic: 9 \n",
      "Words: 0.022*\"cnn\" + 0.016*\"happen\" + 0.011*\"fakenews\" + 0.010*\"attack\" + 0.009*\"yes\" + 0.008*\"jam\" + 0.007*\"youtube\" + 0.006*\"good\" + 0.006*\"read\" + 0.005*\"support\"\n",
      "Topic: 10 \n",
      "Words: 0.020*\"mar\" + 0.019*\"maga\" + 0.017*\"follow\" + 0.014*\"year\" + 0.014*\"election\" + 0.009*\"twitter\" + 0.008*\"thank\" + 0.008*\"old\" + 0.008*\"donald\" + 0.008*\"retweet\"\n",
      "Topic: 11 \n",
      "Words: 0.026*\"lie\" + 0.013*\"americans\" + 0.013*\"bernie\" + 0.011*\"lynch\" + 0.010*\"comey\" + 0.010*\"sanders\" + 0.009*\"fbi\" + 0.009*\"believe\" + 0.009*\"tell\" + 0.009*\"government\"\n",
      "Topic: 12 \n",
      "Words: 0.025*\"shoot\" + 0.014*\"man\" + 0.012*\"antifa\" + 0.010*\"kill\" + 0.010*\"cop\" + 0.007*\"violence\" + 0.007*\"hack\" + 0.006*\"supporters\" + 0.006*\"officer\" + 0.006*\"condemn\"\n",
      "Topic: 13 \n",
      "Words: 0.023*\"media\" + 0.008*\"russia\" + 0.006*\"lol\" + 0.006*\"american\" + 0.006*\"pro\" + 0.006*\"tweet\" + 0.006*\"social\" + 0.006*\"viral\" + 0.005*\"fake\" + 0.005*\"chelsea\"\n",
      "Topic: 14 \n",
      "Words: 0.018*\"pay\" + 0.008*\"stop\" + 0.008*\"cop\" + 0.007*\"plan\" + 0.007*\"tax\" + 0.007*\"arrest\" + 0.007*\"baby\" + 0.006*\"california\" + 0.006*\"man\" + 0.006*\"officer\"\n",
      "Topic: 15 \n",
      "Words: 0.016*\"work\" + 0.010*\"hillaryclinton\" + 0.010*\"talk\" + 0.008*\"night\" + 0.006*\"live\" + 0.005*\"israel\" + 0.005*\"sport\" + 0.005*\"respect\" + 0.005*\"party\" + 0.004*\"tomorrow\"\n",
      "Topic: 16 \n",
      "Words: 0.015*\"god\" + 0.014*\"happy\" + 0.013*\"truth\" + 0.012*\"join\" + 0.011*\"fight\" + 0.009*\"good\" + 0.009*\"bless\" + 0.008*\"learn\" + 0.008*\"stand\" + 0.008*\"patriots\"\n",
      "Topic: 17 \n",
      "Words: 0.014*\"nfl\" + 0.008*\"anthem\" + 0.008*\"protest\" + 0.007*\"man\" + 0.007*\"call\" + 0.007*\"national\" + 0.007*\"hold\" + 0.007*\"senator\" + 0.006*\"secret\" + 0.005*\"service\"\n",
      "Topic: 18 \n",
      "Words: 0.012*\"leave\" + 0.011*\"women\" + 0.011*\"come\" + 0.007*\"real\" + 0.006*\"stay\" + 0.006*\"let\" + 0.005*\"bush\" + 0.005*\"safe\" + 0.004*\"isn\" + 0.004*\"lol\"\n",
      "Topic: 19 \n",
      "Words: 0.013*\"call\" + 0.011*\"years\" + 0.008*\"state\" + 0.008*\"debalwaystrump\" + 0.007*\"kill\" + 0.006*\"things\" + 0.006*\"islamic\" + 0.005*\"hop\" + 0.005*\"terrorists\" + 0.005*\"local\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LDA using TF-IDF\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(doc_term_matrix, num_topics=20, id2word=dictionary, iterations=100, decay=0.7,  gamma_threshold=0.0005, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.009*\"foxnews\" + 0.009*\"christmas\" + 0.008*\"best\" + 0.007*\"spend\" + 0.007*\"meet\" + 0.007*\"sessions\" + 0.007*\"open\" + 0.006*\"king\" + 0.006*\"guess\" + 0.006*\"start\"\n",
      "Topic: 1 \n",
      "Words: 0.010*\"nfl\" + 0.010*\"order\" + 0.009*\"work\" + 0.009*\"texas\" + 0.008*\"happen\" + 0.007*\"vegas\" + 0.007*\"sign\" + 0.006*\"wait\" + 0.006*\"good\" + 0.006*\"team\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"matter\" + 0.012*\"true\" + 0.009*\"soros\" + 0.009*\"anti\" + 0.009*\"judge\" + 0.008*\"stand\" + 0.008*\"country\" + 0.007*\"justice\" + 0.006*\"train\" + 0.006*\"blacktwitter\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"refugees\" + 0.010*\"islamkills\" + 0.008*\"islam\" + 0.007*\"demndebate\" + 0.007*\"demdebate\" + 0.007*\"muslim\" + 0.007*\"woman\" + 0.007*\"syria\" + 0.007*\"muslims\" + 0.007*\"terrorists\"\n",
      "Topic: 4 \n",
      "Words: 0.052*\"cnn\" + 0.019*\"fakenews\" + 0.008*\"attack\" + 0.007*\"democrats\" + 0.006*\"hurt\" + 0.005*\"officials\" + 0.005*\"worst\" + 0.005*\"stand\" + 0.005*\"leave\" + 0.005*\"enjoy\"\n",
      "Topic: 5 \n",
      "Words: 0.019*\"shoot\" + 0.018*\"year\" + 0.018*\"kill\" + 0.015*\"old\" + 0.015*\"school\" + 0.012*\"cop\" + 0.010*\"officer\" + 0.009*\"girl\" + 0.008*\"high\" + 0.007*\"chicago\"\n",
      "Topic: 6 \n",
      "Words: 0.036*\"thank\" + 0.020*\"follow\" + 0.012*\"fake\" + 0.010*\"today\" + 0.009*\"forget\" + 0.008*\"hate\" + 0.008*\"twitter\" + 0.008*\"debalwaystrump\" + 0.008*\"ready\" + 0.008*\"illegal\"\n",
      "Topic: 7 \n",
      "Words: 0.019*\"korea\" + 0.019*\"north\" + 0.010*\"water\" + 0.008*\"dog\" + 0.007*\"photo\" + 0.007*\"wow\" + 0.006*\"try\" + 0.005*\"epic\" + 0.005*\"internet\" + 0.005*\"ella\"\n",
      "Topic: 8 \n",
      "Words: 0.017*\"real\" + 0.016*\"tell\" + 0.011*\"american\" + 0.010*\"job\" + 0.009*\"racist\" + 0.008*\"build\" + 0.007*\"paul\" + 0.006*\"home\" + 0.006*\"rice\" + 0.006*\"music\"\n",
      "Topic: 9 \n",
      "Words: 0.019*\"great\" + 0.010*\"party\" + 0.009*\"end\" + 0.008*\"lol\" + 0.008*\"potus\" + 0.008*\"mind\" + 0.007*\"talk\" + 0.007*\"money\" + 0.006*\"message\" + 0.006*\"war\"\n",
      "Topic: 10 \n",
      "Words: 0.013*\"pay\" + 0.008*\"sport\" + 0.007*\"fuck\" + 0.007*\"baby\" + 0.007*\"wikileaks\" + 0.007*\"tax\" + 0.006*\"return\" + 0.006*\"youtube\" + 0.006*\"money\" + 0.005*\"future\"\n",
      "Topic: 11 \n",
      "Words: 0.019*\"gun\" + 0.011*\"cruz\" + 0.008*\"mueller\" + 0.008*\"house\" + 0.008*\"comey\" + 0.007*\"comment\" + 0.006*\"morning\" + 0.006*\"ted\" + 0.006*\"dream\" + 0.006*\"control\"\n",
      "Topic: 12 \n",
      "Words: 0.012*\"plan\" + 0.010*\"history\" + 0.009*\"immigration\" + 0.008*\"fbi\" + 0.007*\"crime\" + 0.007*\"make\" + 0.007*\"border\" + 0.006*\"chance\" + 0.006*\"act\" + 0.006*\"cia\"\n",
      "Topic: 13 \n",
      "Words: 0.019*\"americans\" + 0.014*\"life\" + 0.012*\"poll\" + 0.012*\"gop\" + 0.010*\"mccain\" + 0.010*\"john\" + 0.009*\"obamacare\" + 0.008*\"lose\" + 0.007*\"push\" + 0.006*\"republicans\"\n",
      "Topic: 14 \n",
      "Words: 0.035*\"tcot\" + 0.025*\"pjnet\" + 0.021*\"women\" + 0.015*\"ccot\" + 0.012*\"hillaryclinton\" + 0.010*\"truth\" + 0.009*\"wakeupamerica\" + 0.009*\"speak\" + 0.009*\"campaign\" + 0.009*\"trumptrain\"\n",
      "Topic: 15 \n",
      "Words: 0.022*\"believe\" + 0.015*\"bernie\" + 0.014*\"enlist\" + 0.012*\"sanders\" + 0.010*\"army\" + 0.009*\"question\" + 0.008*\"patriot\" + 0.008*\"answer\" + 0.008*\"peace\" + 0.008*\"world\"\n",
      "Topic: 16 \n",
      "Words: 0.024*\"god\" + 0.015*\"good\" + 0.012*\"mar\" + 0.011*\"bless\" + 0.009*\"days\" + 0.009*\"join\" + 0.009*\"hard\" + 0.008*\"fight\" + 0.008*\"work\" + 0.007*\"care\"\n",
      "Topic: 17 \n",
      "Words: 0.025*\"play\" + 0.014*\"nowplaying\" + 0.012*\"deal\" + 0.009*\"hack\" + 0.008*\"beat\" + 0.006*\"press\" + 0.006*\"interest\" + 0.006*\"feat\" + 0.005*\"click\" + 0.005*\"soundcloud\"\n",
      "Topic: 18 \n",
      "Words: 0.012*\"target\" + 0.012*\"attack\" + 0.011*\"retweet\" + 0.009*\"rally\" + 0.008*\"isis\" + 0.008*\"antifa\" + 0.008*\"flag\" + 0.007*\"mayor\" + 0.007*\"account\" + 0.006*\"agree\"\n",
      "Topic: 19 \n",
      "Words: 0.018*\"change\" + 0.011*\"gopdebate\" + 0.009*\"fraud\" + 0.007*\"climate\" + 0.007*\"house\" + 0.007*\"voter\" + 0.007*\"gop\" + 0.007*\"vegasgopdebate\" + 0.007*\"senate\" + 0.006*\"iran\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(num_words=10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"pjnet\" + 0.017*\"job\" + 0.015*\"great\" + 0.012*\"ccot\" + 0.012*\"retweet\" + 0.011*\"illegal\" + 0.010*\"things\" + 0.009*\"trumptrain\" + 0.007*\"damn\" + 0.007*\"strong\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"korea\" + 0.018*\"north\" + 0.015*\"isis\" + 0.014*\"wrong\" + 0.013*\"target\" + 0.012*\"islam\" + 0.011*\"muslims\" + 0.009*\"law\" + 0.008*\"water\" + 0.006*\"perfect\"\n",
      "Topic: 2 \n",
      "Words: 0.018*\"mar\" + 0.010*\"claim\" + 0.010*\"bernie\" + 0.008*\"sanders\" + 0.007*\"drug\" + 0.007*\"worst\" + 0.007*\"border\" + 0.006*\"security\" + 0.006*\"terrorism\" + 0.005*\"candidate\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"years\" + 0.007*\"dog\" + 0.007*\"rice\" + 0.006*\"secret\" + 0.006*\"sexual\" + 0.005*\"imo\" + 0.005*\"susan\" + 0.005*\"assault\" + 0.005*\"worry\" + 0.005*\"ago\"\n",
      "Topic: 4 \n",
      "Words: 0.012*\"life\" + 0.009*\"agree\" + 0.008*\"dems\" + 0.007*\"jeff\" + 0.006*\"neverhillary\" + 0.006*\"sessions\" + 0.006*\"hate\" + 0.006*\"blame\" + 0.006*\"voters\" + 0.005*\"travel\"\n",
      "Topic: 5 \n",
      "Words: 0.019*\"music\" + 0.012*\"listen\" + 0.011*\"play\" + 0.010*\"supporters\" + 0.009*\"wow\" + 0.008*\"check\" + 0.007*\"hop\" + 0.007*\"rap\" + 0.007*\"radio\" + 0.007*\"hiphop\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"thank\" + 0.017*\"happy\" + 0.016*\"deal\" + 0.013*\"follow\" + 0.009*\"ready\" + 0.008*\"line\" + 0.007*\"mad\" + 0.007*\"deserve\" + 0.006*\"iran\" + 0.006*\"twitter\"\n",
      "Topic: 7 \n",
      "Words: 0.018*\"money\" + 0.012*\"help\" + 0.010*\"die\" + 0.008*\"plan\" + 0.008*\"cop\" + 0.007*\"christmas\" + 0.006*\"soros\" + 0.006*\"george\" + 0.006*\"use\" + 0.005*\"learn\"\n",
      "Topic: 8 \n",
      "Words: 0.015*\"best\" + 0.009*\"book\" + 0.007*\"speak\" + 0.006*\"joke\" + 0.006*\"racist\" + 0.006*\"truth\" + 0.006*\"weekend\" + 0.006*\"land\" + 0.006*\"stand\" + 0.005*\"brown\"\n",
      "Topic: 9 \n",
      "Words: 0.018*\"russia\" + 0.017*\"fbi\" + 0.011*\"mueller\" + 0.011*\"house\" + 0.010*\"comey\" + 0.009*\"congress\" + 0.008*\"obamacare\" + 0.008*\"pro\" + 0.008*\"fail\" + 0.007*\"vegas\"\n",
      "Topic: 10 \n",
      "Words: 0.026*\"god\" + 0.016*\"real\" + 0.012*\"life\" + 0.009*\"beat\" + 0.009*\"bless\" + 0.008*\"lose\" + 0.008*\"start\" + 0.008*\"best\" + 0.007*\"thank\" + 0.007*\"friends\"\n",
      "Topic: 11 \n",
      "Words: 0.022*\"year\" + 0.016*\"old\" + 0.013*\"school\" + 0.012*\"refugees\" + 0.010*\"high\" + 0.010*\"islamkills\" + 0.010*\"take\" + 0.008*\"pjnet\" + 0.006*\"brussels\" + 0.005*\"wakeupamerica\"\n",
      "Topic: 12 \n",
      "Words: 0.028*\"shoot\" + 0.016*\"antifa\" + 0.011*\"gun\" + 0.011*\"cop\" + 0.011*\"officer\" + 0.010*\"dead\" + 0.010*\"arrest\" + 0.009*\"chicago\" + 0.009*\"free\" + 0.009*\"attack\"\n",
      "Topic: 13 \n",
      "Words: 0.011*\"gopdebate\" + 0.010*\"question\" + 0.008*\"nfl\" + 0.008*\"anthem\" + 0.008*\"trust\" + 0.008*\"military\" + 0.007*\"joe\" + 0.007*\"vegasgopdebate\" + 0.007*\"national\" + 0.006*\"sheriff\"\n",
      "Topic: 14 \n",
      "Words: 0.019*\"history\" + 0.008*\"cruz\" + 0.007*\"surprise\" + 0.007*\"ted\" + 0.007*\"head\" + 0.007*\"hannity\" + 0.007*\"freedom\" + 0.007*\"muslim\" + 0.006*\"enlist\" + 0.006*\"threat\"\n",
      "Topic: 15 \n",
      "Words: 0.047*\"cnn\" + 0.021*\"fakenews\" + 0.010*\"women\" + 0.008*\"foxnews\" + 0.008*\"reveal\" + 0.006*\"rip\" + 0.006*\"hat\" + 0.005*\"stupid\" + 0.005*\"excuse\" + 0.005*\"mcconnell\"\n",
      "Topic: 16 \n",
      "Words: 0.011*\"twitter\" + 0.010*\"post\" + 0.010*\"hillaryclinton\" + 0.009*\"donald\" + 0.009*\"tweet\" + 0.008*\"email\" + 0.008*\"true\" + 0.008*\"fake\" + 0.007*\"dnc\" + 0.007*\"debalwaystrump\"\n",
      "Topic: 17 \n",
      "Words: 0.016*\"care\" + 0.016*\"change\" + 0.015*\"stand\" + 0.013*\"forget\" + 0.010*\"enlist\" + 0.009*\"mccain\" + 0.008*\"talk\" + 0.007*\"judge\" + 0.007*\"weinstein\" + 0.007*\"harvey\"\n",
      "Topic: 18 \n",
      "Words: 0.012*\"protect\" + 0.008*\"nowplaying\" + 0.008*\"prove\" + 0.007*\"happen\" + 0.007*\"step\" + 0.007*\"moore\" + 0.007*\"eric\" + 0.007*\"clear\" + 0.006*\"forward\" + 0.006*\"remember\"\n",
      "Topic: 19 \n",
      "Words: 0.011*\"pay\" + 0.010*\"sure\" + 0.010*\"demndebate\" + 0.010*\"demdebate\" + 0.010*\"court\" + 0.008*\"fraud\" + 0.007*\"morning\" + 0.007*\"voter\" + 0.006*\"corruption\" + 0.006*\"patriots\"\n"
     ]
    }
   ],
   "source": [
    "# another example of topic result using tf-idf\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_words=10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 25 \n",
      "Words: 0.027*\"government\" + 0.022*\"game\" + 0.022*\"million\" + 0.019*\"freedom\" + 0.013*\"play\" + 0.011*\"number\" + 0.009*\"hat\" + 0.008*\"problem\" + 0.008*\"constitution\" + 0.008*\"enlist\"\n",
      "Topic: 13 \n",
      "Words: 0.024*\"antifa\" + 0.013*\"forget\" + 0.009*\"comey\" + 0.009*\"jam\" + 0.009*\"blm\" + 0.008*\"respect\" + 0.007*\"intelligence\" + 0.006*\"attack\" + 0.006*\"violence\" + 0.006*\"house\"\n",
      "Topic: 38 \n",
      "Words: 0.018*\"hit\" + 0.014*\"season\" + 0.013*\"phone\" + 0.013*\"deep\" + 0.010*\"wanna\" + 0.009*\"veteran\" + 0.008*\"threaten\" + 0.008*\"chief\" + 0.007*\"secretary\" + 0.007*\"marine\"\n",
      "Topic: 33 \n",
      "Words: 0.021*\"voters\" + 0.014*\"imo\" + 0.013*\"base\" + 0.012*\"brother\" + 0.009*\"establishment\" + 0.009*\"admin\" + 0.008*\"carolina\" + 0.008*\"reject\" + 0.007*\"coach\" + 0.007*\"policies\"\n",
      "Topic: 6 \n",
      "Words: 0.023*\"country\" + 0.020*\"retweet\" + 0.016*\"sport\" + 0.016*\"best\" + 0.014*\"music\" + 0.014*\"beautiful\" + 0.013*\"soundcloud\" + 0.013*\"rock\" + 0.011*\"click\" + 0.011*\"play\"\n",
      "Topic: 12 \n",
      "Words: 0.026*\"illegal\" + 0.016*\"west\" + 0.011*\"kaepernick\" + 0.011*\"immigrants\" + 0.009*\"pence\" + 0.009*\"gop\" + 0.008*\"alien\" + 0.008*\"exactly\" + 0.008*\"paris\" + 0.008*\"pay\"\n",
      "Topic: 27 \n",
      "Words: 0.024*\"obamacare\" + 0.019*\"water\" + 0.012*\"repeal\" + 0.010*\"unite\" + 0.008*\"draintheswamp\" + 0.008*\"gift\" + 0.007*\"likely\" + 0.007*\"resist\" + 0.007*\"nodapl\" + 0.006*\"traitor\"\n",
      "Topic: 2 \n",
      "Words: 0.025*\"wall\" + 0.020*\"border\" + 0.016*\"build\" + 0.014*\"street\" + 0.013*\"global\" + 0.011*\"different\" + 0.010*\"term\" + 0.010*\"cities\" + 0.010*\"wouldn\" + 0.009*\"rip\"\n",
      "Topic: 4 \n",
      "Words: 0.060*\"god\" + 0.020*\"bless\" + 0.015*\"christmas\" + 0.015*\"fan\" + 0.013*\"office\" + 0.010*\"thank\" + 0.009*\"bannon\" + 0.007*\"ways\" + 0.007*\"seanhannity\" + 0.007*\"victory\"\n",
      "Topic: 37 \n",
      "Words: 0.019*\"fight\" + 0.016*\"heart\" + 0.014*\"judge\" + 0.013*\"islam\" + 0.012*\"kelly\" + 0.011*\"men\" + 0.011*\"ryan\" + 0.010*\"sessions\" + 0.009*\"women\" + 0.009*\"administration\"\n",
      "Topic: 21 \n",
      "Words: 0.046*\"gun\" + 0.022*\"deal\" + 0.020*\"iran\" + 0.017*\"charge\" + 0.016*\"control\" + 0.015*\"cop\" + 0.014*\"shoot\" + 0.011*\"laws\" + 0.011*\"push\" + 0.009*\"nuclear\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"beat\" + 0.024*\"check\" + 0.021*\"shit\" + 0.015*\"music\" + 0.013*\"michael\" + 0.011*\"moore\" + 0.010*\"issue\" + 0.010*\"feature\" + 0.009*\"hiphop\" + 0.009*\"straight\"\n",
      "Topic: 0 \n",
      "Words: 0.047*\"plan\" + 0.024*\"isis\" + 0.023*\"war\" + 0.012*\"shock\" + 0.011*\"free\" + 0.011*\"islamic\" + 0.010*\"syrian\" + 0.010*\"accept\" + 0.009*\"parenthood\" + 0.009*\"daily\"\n",
      "Topic: 26 \n",
      "Words: 0.027*\"join\" + 0.026*\"enlist\" + 0.020*\"army\" + 0.020*\"patriots\" + 0.018*\"patriot\" + 0.016*\"save\" + 0.013*\"fight\" + 0.012*\"veterans\" + 0.011*\"stand\" + 0.011*\"funny\"\n",
      "Topic: 8 \n",
      "Words: 0.066*\"pjnet\" + 0.033*\"ccot\" + 0.028*\"read\" + 0.021*\"wakeupamerica\" + 0.012*\"tomorrow\" + 0.011*\"christian\" + 0.010*\"david\" + 0.009*\"tedcruz\" + 0.009*\"cruzcrew\" + 0.009*\"paul\"\n",
      "Topic: 32 \n",
      "Words: 0.034*\"party\" + 0.012*\"second\" + 0.011*\"write\" + 0.011*\"laugh\" + 0.011*\"middle\" + 0.009*\"ella\" + 0.009*\"soldier\" + 0.009*\"pelosi\" + 0.009*\"christians\" + 0.009*\"rich\"\n",
      "Topic: 5 \n",
      "Words: 0.041*\"year\" + 0.037*\"old\" + 0.014*\"dead\" + 0.011*\"girl\" + 0.011*\"boy\" + 0.011*\"vegas\" + 0.009*\"sorry\" + 0.009*\"mom\" + 0.009*\"tear\" + 0.009*\"nowplaying\"\n",
      "Topic: 39 \n",
      "Words: 0.077*\"cnn\" + 0.035*\"fakenews\" + 0.026*\"yes\" + 0.014*\"trumptrain\" + 0.012*\"ben\" + 0.011*\"carson\" + 0.009*\"hot\" + 0.008*\"leadership\" + 0.008*\"lmao\" + 0.007*\"daca\"\n",
      "Topic: 20 \n",
      "Words: 0.024*\"wait\" + 0.022*\"job\" + 0.013*\"gonna\" + 0.011*\"carry\" + 0.009*\"ticket\" + 0.009*\"europe\" + 0.008*\"view\" + 0.008*\"reality\" + 0.008*\"wikileaks\" + 0.007*\"london\"\n",
      "Topic: 1 \n",
      "Words: 0.041*\"twitter\" + 0.040*\"follow\" + 0.027*\"thank\" + 0.020*\"ready\" + 0.016*\"stop\" + 0.012*\"play\" + 0.011*\"hop\" + 0.010*\"press\" + 0.010*\"radio\" + 0.009*\"challenge\"\n"
     ]
    }
   ],
   "source": [
    "# another example of topic result using tf-idf\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_words=10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
